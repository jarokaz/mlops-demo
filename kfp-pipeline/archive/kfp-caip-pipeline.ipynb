{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestraining training and deployment of scikit-learn model with Kubeflow Pipelines and Cloud AI Platform. \n",
    "\n",
    "In this lab you develop the KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train and deploy a **scikit-learn** model. The lab uses the [Covertype Dat Set](../datasets/covertype/README.md). The model is a multi-class classification model that predicts the type of forest cover from cartographic data. \n",
    "\n",
    "The source data is in BigQuery. The pipeline uses BigQuery to prepare training and evaluation splits, AI Platform Training to run a custom container with data preprocessing and training code, and AI Platform Prediction as a deployment target. The below diagram represents the workflow orchestrated by the pipeline.\n",
    "\n",
    "![Training pipeline](../images/kfp-caip.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings\n",
    "Make sure to update the constants to reflect your environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-workshop'\n",
    "DATASET_LOCATION = 'US'\n",
    "CLUSTER_NAME = 'mlops-workshop-cluster'\n",
    "CLUSTER_ZONE = 'us-central1-a'\n",
    "REGION = 'us-central1'\n",
    "DATASET_ID = 'lab_12'\n",
    "SOURCE_TABLE_ID = 'covertype'\n",
    "SPLITS_TABLE_ID = 'splits'\n",
    "LAB_GCS_BUCKET='gs://mlops-workshop-lab-12'\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset \n",
    "Use BigQuery Python client library to query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID, location=DATASET_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and display 100 rows from the source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))\n",
    "df = client.query(query).to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training, validation, and testing splits\n",
    "#### Prepare the data splitting query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *, \n",
    "CASE(MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10))\n",
    "  WHEN 9 THEN 'test'\n",
    "  WHEN 8 THEN 'validation'\n",
    "  ELSE 'training' END AS Split_Col\n",
    "from `{{ source_table }}` as cover\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SOURCE_TABLE_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the data splitting job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(DATASET_ID)\n",
    "splits_table_ref = dataset_ref.table(SPLITS_TABLE_ID)\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "job_config.destination = splits_table_ref\n",
    "\n",
    "query_job = client.query(query, job_config)\n",
    "query_job.result() # Wait for query to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the table with splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT Cover_Type, Split_Col \n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SPLITS_TABLE_ID))\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the training script\n",
    "#### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *  \n",
    "FROM `{{ source_table }}`\n",
    "WHERE Split_Col in ('training', 'validation')\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, SPLITS_TABLE_ID))\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "df_train = df[df.Split_Col == 'training'].drop('Split_Col', axis=1)\n",
    "df_validation = df[df.Split_Col == 'validation'].drop('Split_Col', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68509501 0.7153876  0.71443346 0.69855428 0.71345155 0.72286145\n",
      " 0.7177117  0.7046909  0.70216342 0.70334217]\n",
      "*********\n",
      "0.707769154705924\n"
     ]
    }
   ],
   "source": [
    "numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "alpha = 0.0001\n",
    "max_iter = 1000\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(alpha=alpha, max_iter=max_iter, loss='log'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Cover_Type', axis=1)\n",
    "y_train = df_train['Cover_Type']\n",
    "X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "y_validation = df_validation['Cover_Type']\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on AI Platform Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a hyperparameter tuning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, project_id, dataset_id, table_id, alpha, max_iter, dataset_location='US'):\n",
    "    \n",
    "  query_template = \"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{{ source_table }}`\n",
    "    WHERE Split_Col in ('training', 'validation')\n",
    "    \"\"\"\n",
    "\n",
    "  source_table='{}.{}.{}'.format(project_id, dataset_id, table_id)\n",
    "  query = Template(query_template).render(\n",
    "    source_table=source_table)\n",
    "\n",
    "  client = bigquery.Client(project=project_id, location=dataset_location)\n",
    "\n",
    "  logging.info('Reading data from BigQuery table: {}'.format(source_table))\n",
    "  df = client.query(query).to_dataframe()\n",
    "  df_train = df[df.Split_Col == 'training'].drop('Split_Col', axis=1)\n",
    "  df_validation = df[df.Split_Col == 'validation'].drop('Split_Col', axis=1)\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(alpha=alpha, max_iter=max_iter, loss='log'))\n",
    "  ])\n",
    "\n",
    "  logging.info('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "  y_validation = df_validation['Cover_Type']\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  accuracy = pipeline.score(X_validation, y_validation)\n",
    "  logging.info('Finished training. Model accuracy: {}'.format(accuracy))\n",
    "    \n",
    "  # Log it with hypertune\n",
    "  hpt = hypertune.HyperTune()\n",
    "  hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='accuracy',\n",
    "    metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  model_filename = 'model.joblib'\n",
    "  joblib.dump(value=pipeline, filename=model_filename)\n",
    "  gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "  subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "  logging.info(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  logging.basicConfig(level=logging.INFO)\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package the script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.9 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-workshop_cloudbuild/source/1576098389.71-d585e18e0b01433c8e3ca751aba78a9a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-workshop/builds/2661c0a3-ae10-41a2-b15a-c0206f2e9cd5].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/2661c0a3-ae10-41a2-b15a-c0206f2e9cd5?project=745302968357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"2661c0a3-ae10-41a2-b15a-c0206f2e9cd5\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-workshop_cloudbuild/source/1576098389.71-d585e18e0b01433c8e3ca751aba78a9a.tgz#1576098390023138\n",
      "Copying gs://mlops-workshop_cloudbuild/source/1576098389.71-d585e18e0b01433c8e3ca751aba78a9a.tgz#1576098390023138...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "221c4376244e: Pulling fs layer\n",
      "6be10f944cd9: Pulling fs layer\n",
      "34c517f627e3: Pulling fs layer\n",
      "8bc377099823: Pulling fs layer\n",
      "f28fcd8ca9f0: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "221c4376244e: Waiting\n",
      "6be10f944cd9: Waiting\n",
      "34c517f627e3: Waiting\n",
      "8bc377099823: Waiting\n",
      "f28fcd8ca9f0: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "6be10f944cd9: Verifying Checksum\n",
      "6be10f944cd9: Download complete\n",
      "34c517f627e3: Verifying Checksum\n",
      "34c517f627e3: Download complete\n",
      "8bc377099823: Verifying Checksum\n",
      "8bc377099823: Download complete\n",
      "f28fcd8ca9f0: Verifying Checksum\n",
      "f28fcd8ca9f0: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "1d7154118978: Download complete\n",
      "35c102085707: Pull complete\n",
      "221c4376244e: Verifying Checksum\n",
      "221c4376244e: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "221c4376244e: Pull complete\n",
      "6be10f944cd9: Pull complete\n",
      "34c517f627e3: Pull complete\n",
      "8bc377099823: Pull complete\n",
      "f28fcd8ca9f0: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "Digest: sha256:848d51a70c3608c4acd37c3dd5a5bacef9c6a51aab5b0064daf5d4258237ef62\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 8f1066e7fc0b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in f0dab4c3a0f1\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/81/10517bad9cb33f954e393ab432165bd4ea9cef3446c5a12f85ec7546c18a/cloudml-hypertune-0.1.0.dev5.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.7/site-packages (from fire) (1.12.0)\n",
      "Collecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=63f0b7fa16bba6541feacbcc0808f93789107aed3f6c9ff550e7f5670142ecff\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev5-py2.py3-none-any.whl size=3926 sha256=de6b45cf99014c1a48e8e99c5add5574acee95450093151888c7ca2beb24267d\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/c9/ed/f049e7ab403c2c3e3a7d110b9b9300cffca397fab604ae8a99\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=b936efbd3a17b3d7dfe8caafcf9cb2ae28e7e22fafad949a736375bcf59efd53\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev5 fire-0.2.1 termcolor-1.1.0\n",
      "Removing intermediate container f0dab4c3a0f1\n",
      " ---> 019ad68c9e1f\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in aa7157769351\n",
      "Removing intermediate container aa7157769351\n",
      " ---> f97be19e6629\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 58da69964fbc\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in fb5f13acfe89\n",
      "Removing intermediate container fb5f13acfe89\n",
      " ---> 06c244fa9103\n",
      "Successfully built 06c244fa9103\n",
      "Successfully tagged gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "The push refers to repository [gcr.io/mlops-workshop/covertype_trainer]\n",
      "aefbc0f620e4: Preparing\n",
      "14ffefcf9624: Preparing\n",
      "51c942b9e716: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "97d733be068e: Preparing\n",
      "d0ce9f8647d3: Preparing\n",
      "fa4332f1c95c: Preparing\n",
      "cd80b8f8deac: Preparing\n",
      "104fbab0f8e2: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "97d733be068e: Waiting\n",
      "d0ce9f8647d3: Waiting\n",
      "fa4332f1c95c: Waiting\n",
      "cd80b8f8deac: Waiting\n",
      "104fbab0f8e2: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "07a867e0ba2d: Layer already exists\n",
      "092c50747c65: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "97d733be068e: Layer already exists\n",
      "fa4332f1c95c: Layer already exists\n",
      "d0ce9f8647d3: Layer already exists\n",
      "cd80b8f8deac: Layer already exists\n",
      "104fbab0f8e2: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "aefbc0f620e4: Pushed\n",
      "14ffefcf9624: Pushed\n",
      "51c942b9e716: Pushed\n",
      "latest: digest: sha256:b73945d07cd6b6b4ef3c60fd39c9e1ac4596f2323b4d29ad9dfe7250b163c73d size: 4283\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                             STATUS\n",
      "2661c0a3-ae10-41a2-b15a-c0206f2e9cd5  2019-12-11T21:06:30+00:00  2M37S     gs://mlops-workshop_cloudbuild/source/1576098389.71-d585e18e0b01433c8e3ca751aba78a9a.tgz  gcr.io/mlops-workshop/covertype_trainer (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME='covertype_trainer'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create hyperparameter configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 10\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_iter\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          500,\n",
    "          1000\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.01\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure and submit hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(LAB_GCS_BUCKET, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "ALPHA = 0.0001\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20191211_212541] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20191211_212541\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20191211_212541\n",
      "jobId: JOB_20191211_212541\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$LAB_GCS_BUCKET/$JOB_NAME \\\n",
    "--master-image-uri=$IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--config $TRAINING_APP_FOLDER/hptuning_config.yaml \\\n",
    "-- \\\n",
    "--project_id=$PROJECT_ID \\\n",
    "--dataset_id=$DATASET_ID \\\n",
    "--table_id=$SPLITS_TABLE_ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-12-11T21:25:44Z'\n",
      "etag: 0yn6EqV4vgs=\n",
      "jobId: JOB_20191211_212541\n",
      "startTime: '2019-12-11T21:25:48Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --project_id=mlops-workshop\n",
      "  - --dataset_id=lab_12\n",
      "  - --table_id=splits\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 3\n",
      "    maxTrials: 10\n",
      "    params:\n",
      "    - discreteValues:\n",
      "      - 500.0\n",
      "      - 1000.0\n",
      "      parameterName: max_iter\n",
      "      type: DISCRETE\n",
      "    - maxValue: 0.01\n",
      "      minValue: 1e-05\n",
      "      parameterName: alpha\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://mlops-workshop-lab-12/JOB_20191211_212541\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/mlops-workshop/covertype_trainer:latest\n",
      "  region: us-central1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20191211_212541?project=mlops-workshop\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2FJOB_20191211_212541&project=mlops-workshop\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2019-12-11 21:25:44 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2019-12-11 21:25:44 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2019-12-11 21:25:44 +0000\tservice\t\tJob JOB_20191211_212541 is queued.\n",
      "INFO\t2019-12-11 21:25:54 +0000\tservice\t2\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:25:54 +0000\tservice\t3\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:25:54 +0000\tservice\t1\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:25:56 +0000\tservice\t1\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:25:57 +0000\tservice\t2\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:25:58 +0000\tservice\t3\tWaiting for training program to start.\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t1\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t1\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t1\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t3\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t3\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:29:00 +0000\tmaster-replica-0\t3\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:29:13 +0000\tmaster-replica-0\t2\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:29:13 +0000\tmaster-replica-0\t2\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:29:13 +0000\tmaster-replica-0\t2\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:30:02 +0000\tmaster-replica-0\t1\tINFO:root:Starting training: alpha=0.006550154491662979, max_iter=500\n",
      "ERROR\t2019-12-11 21:30:04 +0000\tmaster-replica-0\t3\tINFO:root:Starting training: alpha=0.00904765449166298, max_iter=500\n",
      "ERROR\t2019-12-11 21:30:12 +0000\tmaster-replica-0\t1\tINFO:root:Finished training. Model accuracy: 0.6840825212170689\n",
      "ERROR\t2019-12-11 21:30:12 +0000\tmaster-replica-0\t3\tINFO:root:Finished training. Model accuracy: 0.6823137234892936\n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t1\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t1\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t1\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t1\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t3\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "ERROR\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t1\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/1/model.joblib\n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t3\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t3\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t3\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:30:13 +0000\tmaster-replica-0\t3\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/3/model.joblib\n",
      "ERROR\t2019-12-11 21:30:16 +0000\tmaster-replica-0\t2\tINFO:root:Starting training: alpha=0.0015551544916629792, max_iter=1000\n",
      "ERROR\t2019-12-11 21:30:26 +0000\tmaster-replica-0\t2\tINFO:root:Finished training. Model accuracy: 0.6991002942326989\n",
      "INFO\t2019-12-11 21:30:27 +0000\tmaster-replica-0\t2\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:30:27 +0000\tmaster-replica-0\t2\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:27 +0000\tmaster-replica-0\t2\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:30:27 +0000\tmaster-replica-0\t2\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:30:27 +0000\tmaster-replica-0\t2\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/2/model.joblib\n",
      "INFO\t2019-12-11 21:34:26 +0000\tservice\t1\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:34:27 +0000\tservice\t2\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:34:58 +0000\tservice\t3\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:35:18 +0000\tservice\t4\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:35:18 +0000\tservice\t5\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:35:20 +0000\tservice\t6\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:35:21 +0000\tservice\t4\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:35:21 +0000\tservice\t5\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:35:22 +0000\tservice\t6\tWaiting for training program to start.\n",
      "ERROR\t2019-12-11 21:38:31 +0000\tmaster-replica-0\t5\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:38:31 +0000\tmaster-replica-0\t5\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:38:31 +0000\tmaster-replica-0\t5\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t6\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t6\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t6\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t4\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t4\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:38:48 +0000\tmaster-replica-0\t4\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:39:33 +0000\tmaster-replica-0\t5\tINFO:root:Starting training: alpha=0.0006830244886875153, max_iter=500\n",
      "ERROR\t2019-12-11 21:39:46 +0000\tmaster-replica-0\t5\tINFO:root:Finished training. Model accuracy: 0.7066006769052843\n",
      "INFO\t2019-12-11 21:39:47 +0000\tmaster-replica-0\t5\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:39:48 +0000\tmaster-replica-0\t5\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:39:48 +0000\tmaster-replica-0\t5\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:39:48 +0000\tmaster-replica-0\t5\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:39:48 +0000\tmaster-replica-0\t5\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/5/model.joblib\n",
      "ERROR\t2019-12-11 21:39:50 +0000\tmaster-replica-0\t6\tINFO:root:Starting training: alpha=0.0040176289081573485, max_iter=500\n",
      "ERROR\t2019-12-11 21:39:51 +0000\tmaster-replica-0\t4\tINFO:root:Starting training: alpha=0.005678024488687515, max_iter=1000\n",
      "ERROR\t2019-12-11 21:40:01 +0000\tmaster-replica-0\t6\tINFO:root:Finished training. Model accuracy: 0.68787523172951\n",
      "ERROR\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t4\tINFO:root:Finished training. Model accuracy: 0.6857662805925472\n",
      "INFO\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t6\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t6\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t6\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t6\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:40:02 +0000\tmaster-replica-0\t6\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/6/model.joblib\n",
      "INFO\t2019-12-11 21:40:03 +0000\tmaster-replica-0\t4\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:40:03 +0000\tmaster-replica-0\t4\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:40:03 +0000\tmaster-replica-0\t4\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:40:03 +0000\tmaster-replica-0\t4\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:40:03 +0000\tmaster-replica-0\t4\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/4/model.joblib\n",
      "INFO\t2019-12-11 21:42:41 +0000\tservice\t6\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:42:49 +0000\tservice\t4\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:43:51 +0000\tservice\t5\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:44:00 +0000\tservice\t8\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:44:00 +0000\tservice\t7\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:44:02 +0000\tservice\t7\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:44:02 +0000\tservice\t8\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:44:38 +0000\tservice\t9\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:44:40 +0000\tservice\t9\tWaiting for training program to start.\n",
      "ERROR\t2019-12-11 21:47:09 +0000\tmaster-replica-0\t8\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:47:09 +0000\tmaster-replica-0\t8\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:47:09 +0000\tmaster-replica-0\t8\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:47:27 +0000\tmaster-replica-0\t7\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:47:27 +0000\tmaster-replica-0\t7\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:47:27 +0000\tmaster-replica-0\t7\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:47:39 +0000\tmaster-replica-0\t9\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:47:39 +0000\tmaster-replica-0\t9\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:47:39 +0000\tmaster-replica-0\t9\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:48:11 +0000\tmaster-replica-0\t8\tINFO:root:Starting training: alpha=0.0011559299450179153, max_iter=1000\n",
      "ERROR\t2019-12-11 21:48:20 +0000\tmaster-replica-0\t8\tINFO:root:Finished training. Model accuracy: 0.7026548973587088\n",
      "INFO\t2019-12-11 21:48:21 +0000\tmaster-replica-0\t8\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:48:21 +0000\tmaster-replica-0\t8\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:21 +0000\tmaster-replica-0\t8\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:21 +0000\tmaster-replica-0\t8\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:48:21 +0000\tmaster-replica-0\t8\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/8/model.joblib\n",
      "ERROR\t2019-12-11 21:48:30 +0000\tmaster-replica-0\t7\tINFO:root:Starting training: alpha=0.0020077802863627155, max_iter=1000\n",
      "ERROR\t2019-12-11 21:48:40 +0000\tmaster-replica-0\t9\tINFO:root:Starting training: alpha=0.0012248769589059561, max_iter=500\n",
      "ERROR\t2019-12-11 21:48:41 +0000\tmaster-replica-0\t7\tINFO:root:Finished training. Model accuracy: 0.6956987601408235\n",
      "INFO\t2019-12-11 21:48:43 +0000\tmaster-replica-0\t7\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:48:43 +0000\tmaster-replica-0\t7\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:43 +0000\tmaster-replica-0\t7\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:43 +0000\tmaster-replica-0\t7\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:48:43 +0000\tmaster-replica-0\t7\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/7/model.joblib\n",
      "ERROR\t2019-12-11 21:48:51 +0000\tmaster-replica-0\t9\tINFO:root:Finished training. Model accuracy: 0.7023657669608994\n",
      "INFO\t2019-12-11 21:48:52 +0000\tmaster-replica-0\t9\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:48:52 +0000\tmaster-replica-0\t9\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:52 +0000\tmaster-replica-0\t9\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:48:52 +0000\tmaster-replica-0\t9\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:48:52 +0000\tmaster-replica-0\t9\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/9/model.joblib\n",
      "INFO\t2019-12-11 21:51:28 +0000\tservice\t9\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:52:02 +0000\tservice\t7\tJob completed successfully.\n",
      "INFO\t2019-12-11 21:52:21 +0000\tservice\t10\tWaiting for job to be provisioned.\n",
      "INFO\t2019-12-11 21:52:23 +0000\tservice\t10\tWaiting for training program to start.\n",
      "INFO\t2019-12-11 21:52:31 +0000\tservice\t8\tJob completed successfully.\n",
      "ERROR\t2019-12-11 21:55:29 +0000\tmaster-replica-0\t10\t/root/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "ERROR\t2019-12-11 21:55:29 +0000\tmaster-replica-0\t10\t  warnings.warn(msg, category=DeprecationWarning)\n",
      "ERROR\t2019-12-11 21:55:29 +0000\tmaster-replica-0\t10\tINFO:root:Reading data from BigQuery table: mlops-workshop.lab_12.splits\n",
      "ERROR\t2019-12-11 21:56:28 +0000\tmaster-replica-0\t10\tINFO:root:Starting training: alpha=0.0002834346213420818, max_iter=500\n",
      "ERROR\t2019-12-11 21:56:39 +0000\tmaster-replica-0\t10\tINFO:root:Finished training. Model accuracy: 0.7116009320203411\n",
      "INFO\t2019-12-11 21:56:40 +0000\tmaster-replica-0\t10\tCopying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "INFO\t2019-12-11 21:56:40 +0000\tmaster-replica-0\t10\t/ [0 files][    0.0 B/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:56:40 +0000\tmaster-replica-0\t10\t/ [1 files][  7.2 KiB/  7.2 KiB]                                                \n",
      "INFO\t2019-12-11 21:56:40 +0000\tmaster-replica-0\t10\tOperation completed over 1 objects/7.2 KiB.                                      \n",
      "ERROR\t2019-12-11 21:56:40 +0000\tmaster-replica-0\t10\tINFO:root:Saved model in: gs://mlops-workshop-lab-12/JOB_20191211_212541/10/model.joblib\n",
      "INFO\t2019-12-11 22:00:53 +0000\tservice\t10\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
