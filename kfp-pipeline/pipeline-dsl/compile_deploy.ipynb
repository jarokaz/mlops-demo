{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.36\n"
     ]
    }
   ],
   "source": [
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mlops-demo:lab_11' successfully created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r6448a45daee24138_0000016f8c2a07b1_1 ... (10s) Current status: DONE   "
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "PROJECT_ID=mlops-demo\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=lab_11\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/full/covertype.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:INTEGER,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the staging GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "PROJECT_ID=mlops-demo\n",
    "BUCKET_NAME=gs://${PROJECT_ID}-lab-11\n",
    "gsutil mb -p $PROJECT_ID $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a trainer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = 'training_app'\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "  df_train = pd.read_csv(training_dataset_path)\n",
    "  df_validation = pd.read_csv(validation_dataset_path)\n",
    "  if not hptune:\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "  numeric_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "    'Horizontal_Distance_To_Fire_Points']\n",
    "    \n",
    "  categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "  preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features) \n",
    "    ])\n",
    "\n",
    "  pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log'))\n",
    "  ])\n",
    "\n",
    "  print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "  X_train = df_train.drop('Cover_Type', axis=1)\n",
    "  y_train = df_train['Cover_Type']\n",
    "  \n",
    "    \n",
    "  pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  \n",
    "  if hptune:\n",
    "    X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "    y_validation = df_validation['Cover_Type']\n",
    "    accuracy = pipeline.score(X_validation, y_validation)\n",
    "    print('Model accuracy: {}'.format(accuracy))\n",
    "    # Log it with hypertune\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=accuracy\n",
    "    )\n",
    "\n",
    "  # Save the model\n",
    "  if not hptune:\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "    gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "    print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.4 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://mlops-demo_cloudbuild/source/1578605826.62-1a80ecd6629a41ffb767abb9901eeb69.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-demo/builds/5be349a9-5e33-4700-974d-1a3c2e54845c].\n",
      "Logs are available at [https://console.cloud.google.com/gcr/builds/5be349a9-5e33-4700-974d-1a3c2e54845c?project=609934025272].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"5be349a9-5e33-4700-974d-1a3c2e54845c\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-demo_cloudbuild/source/1578605826.62-1a80ecd6629a41ffb767abb9901eeb69.tgz#1578605827100723\n",
      "Copying gs://mlops-demo_cloudbuild/source/1578605826.62-1a80ecd6629a41ffb767abb9901eeb69.tgz#1578605827100723...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "35c102085707: Pulling fs layer\n",
      "251f5509d51d: Pulling fs layer\n",
      "8e829fe70a46: Pulling fs layer\n",
      "6001e1789921: Pulling fs layer\n",
      "1259902c87a2: Pulling fs layer\n",
      "83ca0edf82af: Pulling fs layer\n",
      "a459cc7a0819: Pulling fs layer\n",
      "221c4376244e: Pulling fs layer\n",
      "6be10f944cd9: Pulling fs layer\n",
      "34c517f627e3: Pulling fs layer\n",
      "8bc377099823: Pulling fs layer\n",
      "f28fcd8ca9f0: Pulling fs layer\n",
      "a5d245cced6f: Pulling fs layer\n",
      "8c6be6aa5553: Pulling fs layer\n",
      "1d7154118978: Pulling fs layer\n",
      "1df8626a77b0: Pulling fs layer\n",
      "6001e1789921: Waiting\n",
      "1259902c87a2: Waiting\n",
      "83ca0edf82af: Waiting\n",
      "a459cc7a0819: Waiting\n",
      "221c4376244e: Waiting\n",
      "6be10f944cd9: Waiting\n",
      "34c517f627e3: Waiting\n",
      "8bc377099823: Waiting\n",
      "f28fcd8ca9f0: Waiting\n",
      "a5d245cced6f: Waiting\n",
      "8c6be6aa5553: Waiting\n",
      "1d7154118978: Waiting\n",
      "1df8626a77b0: Waiting\n",
      "251f5509d51d: Verifying Checksum\n",
      "251f5509d51d: Download complete\n",
      "8e829fe70a46: Verifying Checksum\n",
      "8e829fe70a46: Download complete\n",
      "35c102085707: Verifying Checksum\n",
      "35c102085707: Download complete\n",
      "6001e1789921: Verifying Checksum\n",
      "6001e1789921: Download complete\n",
      "a459cc7a0819: Verifying Checksum\n",
      "a459cc7a0819: Download complete\n",
      "83ca0edf82af: Verifying Checksum\n",
      "83ca0edf82af: Download complete\n",
      "6be10f944cd9: Verifying Checksum\n",
      "6be10f944cd9: Download complete\n",
      "34c517f627e3: Verifying Checksum\n",
      "34c517f627e3: Download complete\n",
      "8bc377099823: Verifying Checksum\n",
      "8bc377099823: Download complete\n",
      "f28fcd8ca9f0: Verifying Checksum\n",
      "f28fcd8ca9f0: Download complete\n",
      "a5d245cced6f: Verifying Checksum\n",
      "a5d245cced6f: Download complete\n",
      "8c6be6aa5553: Verifying Checksum\n",
      "8c6be6aa5553: Download complete\n",
      "1d7154118978: Verifying Checksum\n",
      "1d7154118978: Download complete\n",
      "1259902c87a2: Verifying Checksum\n",
      "1259902c87a2: Download complete\n",
      "1df8626a77b0: Verifying Checksum\n",
      "1df8626a77b0: Download complete\n",
      "35c102085707: Pull complete\n",
      "221c4376244e: Verifying Checksum\n",
      "221c4376244e: Download complete\n",
      "251f5509d51d: Pull complete\n",
      "8e829fe70a46: Pull complete\n",
      "6001e1789921: Pull complete\n",
      "1259902c87a2: Pull complete\n",
      "83ca0edf82af: Pull complete\n",
      "a459cc7a0819: Pull complete\n",
      "221c4376244e: Pull complete\n",
      "6be10f944cd9: Pull complete\n",
      "34c517f627e3: Pull complete\n",
      "8bc377099823: Pull complete\n",
      "f28fcd8ca9f0: Pull complete\n",
      "a5d245cced6f: Pull complete\n",
      "8c6be6aa5553: Pull complete\n",
      "1d7154118978: Pull complete\n",
      "1df8626a77b0: Pull complete\n",
      "Digest: sha256:848d51a70c3608c4acd37c3dd5a5bacef9c6a51aab5b0064daf5d4258237ef62\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 8f1066e7fc0b\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 9c6acd6581cc\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: six in /root/miniconda3/lib/python3.7/site-packages (from fire) (1.12.0)\n",
      "Collecting termcolor\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=9b30c9d9e3fba2dac074f3273e0308ce7b796699c737d4623dd94f262970e1bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=7284e1fc09cd65ab84f9d19d32bf98f30ba21e9c0e0f6d6a8e3e01a6a825f3ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=52d40dbf3368cdd91ab4bb436f773c163014d524a01c9f76dee2a2f0bd0f41e3\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, fire, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.2.1 termcolor-1.1.0\n",
      "Removing intermediate container 9c6acd6581cc\n",
      " ---> 3c794f526bc1\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 2cc5a226af8d\n",
      "Removing intermediate container 2cc5a226af8d\n",
      " ---> 1e1341428c31\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 9fa719ff717a\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 79178e7c4993\n",
      "Removing intermediate container 79178e7c4993\n",
      " ---> 2583e976a69c\n",
      "Successfully built 2583e976a69c\n",
      "Successfully tagged gcr.io/mlops-demo/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-demo/trainer_image:latest\n",
      "The push refers to repository [gcr.io/mlops-demo/trainer_image]\n",
      "63b391dcfb56: Preparing\n",
      "da588dd6cc64: Preparing\n",
      "7414787a79a2: Preparing\n",
      "07a867e0ba2d: Preparing\n",
      "092c50747c65: Preparing\n",
      "d6fb36f9bda1: Preparing\n",
      "f36c7efe6784: Preparing\n",
      "97d733be068e: Preparing\n",
      "d0ce9f8647d3: Preparing\n",
      "fa4332f1c95c: Preparing\n",
      "cd80b8f8deac: Preparing\n",
      "104fbab0f8e2: Preparing\n",
      "4019db0181d2: Preparing\n",
      "5a78197acff6: Preparing\n",
      "804e87810c15: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "d6fb36f9bda1: Waiting\n",
      "f36c7efe6784: Waiting\n",
      "97d733be068e: Waiting\n",
      "d0ce9f8647d3: Waiting\n",
      "fa4332f1c95c: Waiting\n",
      "cd80b8f8deac: Waiting\n",
      "104fbab0f8e2: Waiting\n",
      "4019db0181d2: Waiting\n",
      "5a78197acff6: Waiting\n",
      "804e87810c15: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "092c50747c65: Layer already exists\n",
      "07a867e0ba2d: Layer already exists\n",
      "f36c7efe6784: Layer already exists\n",
      "d6fb36f9bda1: Layer already exists\n",
      "d0ce9f8647d3: Layer already exists\n",
      "97d733be068e: Layer already exists\n",
      "fa4332f1c95c: Layer already exists\n",
      "cd80b8f8deac: Layer already exists\n",
      "104fbab0f8e2: Layer already exists\n",
      "4019db0181d2: Layer already exists\n",
      "5a78197acff6: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "804e87810c15: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "da588dd6cc64: Pushed\n",
      "7414787a79a2: Pushed\n",
      "63b391dcfb56: Pushed\n",
      "latest: digest: sha256:3784c2beddc0c490db91e9228bbe77a833d68caa09882c33635bb48ffcfead41 size: 4283\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                IMAGES                                     STATUS\n",
      "5be349a9-5e33-4700-974d-1a3c2e54845c  2020-01-09T21:37:07+00:00  2M33S     gs://mlops-demo_cloudbuild/source/1578605826.62-1a80ecd6629a41ffb767abb9901eeb69.tgz  gcr.io/mlops-demo/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID='mlops-demo'\n",
    "IMAGE_NAME='trainer_image'\n",
    "IMAGE_TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n",
    "\n",
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export PROJECT_ID=mlops-demo\n",
    "export COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/\n",
    "export BASE_IMAGE=gcr.io/deeplearning-platform-release/base-cpu\n",
    "export TRAINER_IMAGE=gcr.io/$PROJECT_ID/trainer_image:latest\n",
    "export RUNTIME_VERSION=1.15\n",
    "export PYTHON_VERSION=3.7\n",
    "\n",
    "dsl-compile --py covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pipeline to KFP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline f8b7f150-e92f-4e3c-a46e-133a5832267a has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           f8b7f150-e92f-4e3c-a46e-133a5832267a\n",
      "Name         covertype_classifier_training\n",
      "Description\n",
      "Uploaded at  2020-01-10T18:29:55+00:00\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| Parameter Name              | Default Value                                    |\n",
      "+=============================+==================================================+\n",
      "| project_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| region                      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| source_table_name           |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| gcs_root                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_name      |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| evaluation_metric_threshold |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| model_id                    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| version_id                  |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| replace_existing_version    |                                                  |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| hypertune_settings          | {                                                |\n",
      "|                             |     \"hyperparameters\":  {                        |\n",
      "|                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "|                             |         \"maxTrials\": 4,                          |\n",
      "|                             |         \"maxParallelTrials\": 2,                  |\n",
      "|                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "|                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "|                             |         \"params\": [                              |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"max_iter\",     |\n",
      "|                             |                 \"type\": \"DISCRETE\",              |\n",
      "|                             |                 \"discreteValues\": [400, 600]     |\n",
      "|                             |             },                                   |\n",
      "|                             |             {                                    |\n",
      "|                             |                 \"parameterName\": \"alpha\",        |\n",
      "|                             |                 \"type\": \"DOUBLE\",                |\n",
      "|                             |                 \"minValue\": 0.0005,              |\n",
      "|                             |                 \"maxValue\": 0.005,               |\n",
      "|                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "|                             |             }                                    |\n",
      "|                             |         ]                                        |\n",
      "|                             |     }                                            |\n",
      "|                             | }                                                |\n",
      "+-----------------------------+--------------------------------------------------+\n",
      "| dataset_location            | US                                               |\n",
      "+-----------------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='covertype_classifier_training'\n",
    "PIPELINE_PACKAGE='covertype_training_pipeline.yaml'\n",
    "INVERSE_PROXY_HOST='1ff32660e53f5d89-dot-us-central1.notebooks.googleusercontent.com'\n",
    "\n",
    "!kfp --endpoint $INVERSE_PROXY_HOST pipeline upload -p $PIPELINE_NAME $PIPELINE_PACKAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                                     | Uploaded at               |\n",
      "+======================================+==========================================================+===========================+\n",
      "| f8b7f150-e92f-4e3c-a46e-133a5832267a | covertype_classifier_training                            | 2020-01-10T18:29:55+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 48b0c023-b881-4ee1-9119-4b479998f142 | popularity_predictor_training                            | 2020-01-10T03:44:39+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| be12500c-2f01-4efe-bf2d-0f24479f217c | [Sample] Basic - Exit Handler                            | 2020-01-09T18:56:04+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 2e130201-0fdf-42d9-8898-22e8b2cf3416 | [Sample] Basic - Conditional execution                   | 2020-01-09T18:56:03+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 68c4181d-91da-4e3d-8fe4-e9e6d97a750d | [Sample] Basic - Parallel execution                      | 2020-01-09T18:56:01+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 534fa461-7495-4659-af51-6a825c033342 | [Sample] Basic - Sequential execution                    | 2020-01-09T18:56:00+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| 0396da79-bf48-453e-b84e-5c8ef79f2f6f | [Sample] Unified DSL - Taxi Tip Prediction Model Trainer | 2020-01-09T18:55:59+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n",
      "| a4b77c27-89ce-4c72-8428-98ce267770ef | [Sample] ML - XGBoost - Training with Confusion Matrix   | 2020-01-09T18:55:58+00:00 |\n",
      "+--------------------------------------+----------------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "INVERSE_PROXY_HOST='1ff32660e53f5d89-dot-us-central1.notebooks.googleusercontent.com'\n",
    "\n",
    "!kfp --endpoint $INVERSE_PROXY_HOST pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2aac3a95-9515-475e-8cbd-b7b56c088208 is submitted\n",
      "+--------------------------------------+------------------+----------+---------------------------+\n",
      "| run id                               | name             | status   | created at                |\n",
      "+======================================+==================+==========+===========================+\n",
      "| 2aac3a95-9515-475e-8cbd-b7b56c088208 | Training_Run_001 |          | 2020-01-10T17:57:41+00:00 |\n",
      "+--------------------------------------+------------------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ID='bef76c5a-2bb4-4190-ab52-20834c78a846'\n",
    "\n",
    "PROJECT_ID='mlops-demo'\n",
    "INVERSE_PROXY_HOST='1ff32660e53f5d89-dot-us-central1.notebooks.googleusercontent.com'\n",
    "STAGING_GCS_BUCKET='gs://mlops-demo-lab-11'\n",
    "RUN_NAME=\"Training_Run_001\"\n",
    "EXPERIMENT_NAME='Covertype_Classifier_Training'\n",
    "\n",
    "!kfp --endpoint $INVERSE_PROXY_HOST run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_NAME \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$STAGING_GCS_BUCKET \\\n",
    "region=us-central1 \\\n",
    "source_table_name=lab_11.covertype \\\n",
    "dataset_id=splits \\\n",
    "evaluation_metric_name=accuracy \\\n",
    "evaluation_metric_threshold=0.69 \\\n",
    "model_id=covertype_classifier \\\n",
    "version_id=v0.1 \\\n",
    "replace_existing_version=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
